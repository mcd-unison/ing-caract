{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<p><img src=\"https://mcd.unison.mx/wp-content/themes/awaken/img/logo_mcd.png\" width=\"150\">\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "# Curso *Ingeniería de Características*\n",
    "\n",
    "### Descargando datos\n",
    "\n",
    "\n",
    "<p> Julio Waissman Vilanova </p>\n",
    "\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/mcd-unison/ing-caract/blob/main/ejemplos/integracion/python/descarga_datos.ipynb\"><img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\" />Ejecuta en Google Colab</a>\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Descargando datos a la fuerza bruta\n",
    "\n",
    "Vamos a ver primero como ir descargando datos y luego como lidiar con diferentes formatos. Es muy importante que, si los datos los vamos a cargar por única vez, descargar el conjunto de datos, tal como se encuentran, esto es `raw data`.\n",
    "\n",
    "Vamos primero cargando las bibliotecas necesarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os  # Para manejo de archivos y directorios\n",
    "import urllib.request # Una forma estandard de descargar datos\n",
    "# import requests # Otra forma no de las librerías de uso comun\n",
    "\n",
    "import datetime # Fecha de descarga\n",
    "import pandas as pd # Solo para ver el archivo descargado\n",
    "import zipfile # Descompresión de archivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante saber en donde nos encontramos y crear los subdirectorios necesarios para guardar los datos de manera ordenada. Tambien es importante evitar cargar datos que ya han sido descargados anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pwd\n",
    "print(os.getcwd())\n",
    "\n",
    "#  Estos son los datos que vamos a descargar y donde vamos a guardarlos\n",
    "desaparecidos_RNPDNO_url = \"http://www.datamx.io/dataset/fdd2ca20-ee70-4a31-9bdf-823f3c1307a2/resource/d352810c-a22e-4d72-bb3b-33c742c799dd/download/desaparecidos3ago.zip\"\n",
    "desaparecidos_RNPDNO_archivo = \"desaparecidosRNPDNO.zip\"\n",
    "desaparecidos_corte_nacional_url = \"http://www.datamx.io/dataset/fdd2ca20-ee70-4a31-9bdf-823f3c1307a2/resource/4865e244-cf59-4d39-b863-96ed7f45cc70/download/nacional.json\"\n",
    "desaparecidos_corte_nacional_archivo = \"desaparecidos_nacional.json\"\n",
    "subdir = \"./data/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(desaparecidos_RNPDNO_archivo):\n",
    "    if not os.path.exists(subdir):\n",
    "        os.makedirs(subdir)\n",
    "    urllib.request.urlretrieve(desaparecidos_RNPDNO_url, subdir + desaparecidos_RNPDNO_archivo)  \n",
    "    with zipfile.ZipFile(subdir + desaparecidos_RNPDNO_archivo, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(subdir)\n",
    "    \n",
    "    urllib.request.urlretrieve(desaparecidos_corte_nacional_url, subdir + desaparecidos_corte_nacional_archivo)  \n",
    "\n",
    "    with open(subdir + \"info.txt\", 'w') as f:\n",
    "        f.write(\"Archivos sobre personas desaparecidas\\n\")\n",
    "        info = \"\"\"\n",
    "        Datos de desaparecidos, corte nacional y desagregación a nivel estatal, \n",
    "        por edad, por sexo, por nacionalidad, por año de desaparición y por mes\n",
    "        de desaparición para los últimos 12 meses.\n",
    "\n",
    "        Los datos se obtuvieron del RNPDNO con fecha de 03 de agosto de 2021\n",
    "        (la base de datos no se ha actualizado últimamente) \n",
    "\n",
    "        \"\"\" \n",
    "        f.write(info + '\\n')\n",
    "        f.write(\"Descargado el \" + datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \"\\n\")\n",
    "        f.write(\"Desde: \" + desaparecidos_RNPDNO_url + \"\\n\")\n",
    "        f.write(\"Nombre: \" + desaparecidos_RNPDNO_archivo + \"\\n\")\n",
    "        f.write(\"Agregados nacionales descargados desde: \" + desaparecidos_corte_nacional_url + \"\\n\")\n",
    "        f.write(\"Nombre: \" + desaparecidos_corte_nacional_archivo + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Archivos en formato `json`\n",
    "\n",
    "Los archivos en formato json son posiblemente los más utilizados actualmente para transferir información por internet, ya que se usa en prácticamente todas las REST API. Como acabamos de ver es normal tener que enfrentarse con archivos `json` pésimamente o nada documentados, por lo que es necesario saber como tratarlos. \n",
    "\n",
    "Vamos a ver como se hace eso utilizando la bibloteca de `json`y la de `pandas`. Para `pandas`les recomiendo, si no lo conocen, de darle una vuelta a [la documentación y los tutoriales](https://pandas.pydata.org/docs/) que está muy bien hecha. O a el [curso básico de Kaggle](https://www.kaggle.com/learn/pandas).\n",
    "\n",
    "Sobre `json`, posiblemente [la página con la especificación](https://www.json.org/json-en.html) sea más que suficiente. \n",
    "\n",
    "Vamos a hacer un ejemplito sencillo y carismático revisando los repositorios de [github](https://github.com) y les voy a dejar que exploren los `json` de los archivos de personas desaparecidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Esto es como una segunda piel\n",
    "import json # Una forma estandar de leer archivos json \n",
    "\n",
    "archivo_url = \"https://api.github.com/users/google/repos\"\n",
    "archivo_nombre = \"repos-google.json\"\n",
    "subdir = \"./data/\"\n",
    "\n",
    "if not os.path.exists(subdir + archivo_nombre):\n",
    "    if not os.path.exists(subdir):\n",
    "        os.makedirs(subdir)\n",
    "    urllib.request.urlretrieve(archivo_url, subdir + archivo_nombre)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos primero a ver como le hacemos con `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repos = pd.read_json(subdir + archivo_nombre)\n",
    "\n",
    "df_repos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repos.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y ahora como le hacemos con la biblioteca de `json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(subdir + archivo_nombre, 'r') as fp:\n",
    "    repos = json.load(fp)\n",
    "\n",
    "print(f\"\\nNúmero de entradas: {len(repos)}\")\n",
    "print(f\"\\nNombre de los atributos: { ', '.join(repos[0].keys())}\")\n",
    "print(f\"\\nAtributos de 'owner': {', '.join(repos[0]['owner'].keys())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio\n",
    "\n",
    "Utiliza los archivos `json` descargados con el detalle a nivel estatal, y genera unos 3 `DataFrame` con información sobre personas desaparecidas dependiendo de diferentes características. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Archivos xml\n",
    "\n",
    "Los archivos *xml* son una manera de compartir información a través de internet o de guardar información con formatos genéricos que sigue siendo muy utilizada hoy en día. En general lidiar con archivos xml es una pesadilla y se necesita explorarlos con calma y revisarlos bien antes de usarlos. \n",
    "\n",
    "La definición del formato y su uso se puede revisar en [este tutorial de la w3schools](https://www.w3schools.com/xml/default.asp). Vamos a ver un ejemplo sencillo basado en la librería [xml.etree.ElementTree](https://docs.python.org/3/library/xml.etree.elementtree.html) que viene de base en python:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as et \n",
    "\n",
    "archivo_url = \"https://github.com/mcd-unison/ing-caract/raw/main/ejemplos/integracion/ejemplos/ejemplo.xml\"\n",
    "archivo_nombre = \"ejemplito.xml\"\n",
    "subdir = \"./data/\"\n",
    "\n",
    "if not os.path.exists(subdir + archivo_nombre):\n",
    "    if not os.path.exists(subdir):\n",
    "        os.makedirs(subdir)\n",
    "    urllib.request.urlretrieve(archivo_url, subdir + archivo_nombre)\n",
    "\n",
    "\n",
    "desayunos = et.parse(subdir + archivo_nombre)\n",
    "\n",
    "for (i, des) in enumerate(desayunos.getroot()):\n",
    "    print(\"Opción {}:\".format(i+1))\n",
    "    for prop in des:\n",
    "        print(\"\\t{}: {}\".format(prop.tag, prop.text.strip()))\n",
    "\n",
    "# Se puede buscar por etiquetas y subetiquetas\n",
    "\n",
    "print(\"Los desayunos disponibles son: \" + \n",
    "      \", \".join([p.text for p in desayunos.findall(\"food/name\")]))\n",
    "\n",
    "# ¿Como se podría poner esta información en un DataFrame de `pandas`?\n",
    "# Agreguen tanto código como consideren necesario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia es un buen ejemplo de un lugar donde la información se guarda y se descarga en forma de archivos xml. Por ejemplo, si queremos descargar datos de la wikipedia [con su herramienta de exportación en python](https://www.mediawiki.org/wiki/Manual:Pywikibot) utilizando [las categorias definidas por Wikipedia](https://es.wikipedia.org/wiki/Portal:Portada).\n",
    "\n",
    "Para descargar los datos de wikipedia, vamos a hacer un uso de la [API de Mediawiki](https://es.wikipedia.org/w/api.php). Utilizando el módulo `requests` de python, podemos hacer una consulta a la API y obtener los datos en formato json. Más adelante vamos a hablar más sobre el uso de APIs para obtención de información.\n",
    "\n",
    "Primero definamos dos funciones, una para consultar el listado de entradas particulares de Wikipedia, y otra para descargar la información necesaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# URL de la API de MediaWiki de Wikipedia en español\n",
    "API_URL = \"https://es.wikipedia.org/w/api.php\"\n",
    "\n",
    "# Cabecera para identificar nuestra aplicación (buena práctica)\n",
    "HEADERS = {\n",
    "    'User-Agent': 'WikiXMLPseudoDump/1.0 (julio.waissman@unison.mx)'\n",
    "}\n",
    "\n",
    "def get_page_titles(category_title):\n",
    "    \"\"\"Obtiene la lista de títulos de páginas de una categoría.\"\"\"\n",
    "    titles = []\n",
    "    cmcontinue = None \n",
    "    while True:\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"list\": \"categorymembers\",\n",
    "            \"cmtitle\": category_title,\n",
    "            \"cmlimit\": \"500\",  # El límite máximo por petición\n",
    "            \"cmcontinue\": cmcontinue\n",
    "        }     \n",
    "        response = requests.get(API_URL, params=params, headers=HEADERS)\n",
    "        data = response.json()\n",
    "        \n",
    "        for member in data['query']['categorymembers']:\n",
    "            titles.append(member['title'])           \n",
    "        if 'continue' in data:\n",
    "            cmcontinue = data['continue']['cmcontinue']\n",
    "        else:\n",
    "            break           \n",
    "    return titles\n",
    "\n",
    "def get_page_content_in_xml(page_titles):\n",
    "    \"\"\"Obtiene el contenido de las páginas en formato XML.\"\"\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"xml\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"rvprop\": \"content\",\n",
    "        \"rvslots\": \"main\",\n",
    "        \"titles\": \"|\".join(page_titles)\n",
    "    }\n",
    "    response = requests.get(API_URL, params=params, headers=HEADERS)\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora usamos las funciones para obtener una lista de poetas argentinos, cada uno en formato `xml`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoria = \"Poetas de Argentina\"\n",
    "    \n",
    "print(f\"Obteniendo la lista de entradas de '{categoria}'...\")\n",
    "titles = get_page_titles('Categoría:'+categoria)\n",
    "\n",
    "print(f\"Se encontraron {len(titles)} entradas.\")\n",
    "if not titles:\n",
    "  raise ValueError(\"No se encontraron páginas en la categoría especificada.\")\n",
    "\n",
    "batch_size = 50 # Lote de títulos para la segunda petición (máximo 50)\n",
    "all_xml_data = []\n",
    "\n",
    "for i in range(0, len(titles), batch_size):\n",
    "  batch_titles = titles[i:i + batch_size]\n",
    "  xml_content = get_page_content_in_xml(batch_titles)\n",
    "  all_xml_data.append(xml_content)\n",
    "  print(f\"Procesadas entradas {i + 1} a {i + batch_size} de {len(titles)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora las juntamos en un solo documento xml y lo guardamos como archivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinar todos los XML en un solo archivo\n",
    "root = et.Element(categoria.lower().replace(\" \", \"_\"))\n",
    "for xml_data in all_xml_data:\n",
    "  # El contenido XML de la API tiene un elemento <api> y dentro <query>, que necesitamos para el contenido\n",
    "  api_root = et.fromstring(xml_data)\n",
    "  query_element = api_root.find('query')\n",
    "  if query_element:\n",
    "    # Los elementos <page> son los que contienen la información de cada poeta\n",
    "    for page_element in query_element.findall('pages/page'):\n",
    "      root.append(page_element)\n",
    "tree = et.ElementTree(root)\n",
    "\n",
    "# Guardamos en un archivo con el mismo nombre que la categoría (con .xml)\n",
    "output_filename = categoria.lower().replace(\" \", \"_\") + \".xml\"\n",
    "with open(output_filename, \"wb\") as f:\n",
    "      # Escribe el XML completo al archivo\n",
    "        tree.write(f, encoding='utf-8', xml_declaration=True)\n",
    "        print(f\"\\nArchivo '{output_filename}' creado exitosamente con la información completa.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora vamos a ver como leer el archivo `xml` y listar el nombre de los poetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poetas = et.parse(output_filename)\n",
    "for poeta in poetas.getroot():\n",
    "    print(poeta.attrib['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio\n",
    "\n",
    "Entender la estructura del archivo `xml` de poetas, hacer un query de otro tema que consideren interesante y generar un `DataFrame` con la información más importante. No olvides de comentar tu código y explicar la estructura del archivo `xml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Archivos de Excel\n",
    "\n",
    "Los archivos de excel son a veces nuestros mejores amigos, y otras veces nuestras peores pesadillas. Un archivo en excel (o cualquier otra hoja de caálculo) son formatos muy útiles que permiten compartir información técnica con personas sin preparación técnica, lo que lo vuelve una herramienta muy poderosa para comunicar hallazgos a los usuarios.\n",
    "\n",
    "Igualmente, la manipulación de datos a través de hojas de cálculo, sin usarlas correctamente 8esto es, programando cualquier modificación) genera normalmente un caos y una fuga de información importante para una posterior toma de desición. \n",
    "\n",
    "Como buena práctica, si se tiene acceso a la fuente primaria de datos y se puede uno evitar el uso de datos procesados en hoja de calculo, siempre es mejor esa alternativa (como científico de datos o analista de datos). Pero eso muchas veces es imposible.\n",
    "\n",
    "Vamos a dejar la importación desde `xlsx` a los cursos de *DataCamp* que lo tratan magistralmente. Es importante que, para que se pueda importar desde python o R, muchas veces es necesario instalar librerías extras."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('caract')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "266c02d0b88fb79ac68216b08bc6bf334e56f5daeb776843302a4ad1205260c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
