{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Copia de nube_informe",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![](https://mcd.unison.mx/wp-content/themes/awaken/img/logo_mcd.png)\n",
        "\n",
        "# Nubes de palabras del 3er Informe de gobierno federal\n",
        "\n",
        "##Ingeniería de Características 2021-2\n",
        "\n",
        "**Julio Waissman**\n",
        "\n",
        "septiembre, 2021\n"
      ],
      "metadata": {
        "id": "HMqV5MG6pkIg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introducción\n",
        "\n",
        "En esta libreta vamos a ver como hacer nubes de palabras como un pretexto para ver como usar la biblioteca `spacy` en la limpieza de lenguaje natural (procesamiento sencillo).\n",
        "\n",
        "\n",
        "Como un ejemplo de aplicación actual (al momento de hacer la libreta, claro). Vamos a utilizar el [Discurso del presidente Andrés Manuel López Obrador durante el Tercer Informe de Gobierno](https://lopezobrador.org.mx/2021/09/01/discurso-del-presidente-andres-manuel-lopez-obrador-durante-el-tercer-informe-de-gobierno/).\n",
        "\n",
        "![](https://lopezobrador.org.mx/wp-content/uploads/2021/09/Tercer-Informe-relieve-2.png)\n",
        "\n",
        "Carguemos primero las bibliotecas que vamos ausar y algunas configuraciones de base.\n",
        "\n"
      ],
      "metadata": {
        "id": "UKTi65X3tqJh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install requests"
      ],
      "outputs": [],
      "metadata": {
        "id": "9yhAUMcvrLhE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import wordcloud\n",
        "\n",
        "nlp = spacy.load('es_core_news_md')\n",
        "plt.style.use('ggplot')\n",
        "plt.rcParams['figure.figsize'] = (15, 7)\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "fx1bbZuzpiE2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descargando el texto\n",
        "\n",
        "Vamos a usar `requests` para descargar la página completa como datos crudos, y luego utilizaremos `BeautifulSoup` para extraer del archivo el texto en parágrafor. Cáda parágrafo lo vamos a guardar en una entrada de un `dataframe`, por si luego decidimos hacer otro tipo de análisis con la información.\n"
      ],
      "metadata": {
        "id": "akWSmNekviV5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "url = \"https://lopezobrador.org.mx/2021/09/01/discurso-del-presidente-andres-manuel-lopez-obrador-durante-el-tercer-informe-de-gobierno/\"\n",
        "\n",
        "informe_html = requests.get(url)\n",
        "sopa = BeautifulSoup(informe_html.text)\n",
        "\n",
        "contenido = sopa.find_all(\"div\", {\"class\":\"entry-content\"})\n",
        "df_informe = pd.DataFrame({\n",
        "    'Parrafo': [parrafo.text for parrafo in contenido[0].find_all(\"p\")] \n",
        "})\n",
        "\n",
        "df_informe"
      ],
      "outputs": [],
      "metadata": {
        "id": "2HW7MkudwGwl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y por lo que vemos, hay al menos uas cuantas lineas que no contienen caractéres alfanumericos, las cuales se usan como separador y podrían ser eliminadas."
      ],
      "metadata": {
        "id": "9LorE9aS058M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df_informe = df_informe[df_informe.Parrafo.str.contains(r\"\\w\", regex=True)]\n",
        "df_informe"
      ],
      "outputs": [],
      "metadata": {
        "id": "-Ntu8PgX1GPi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Haciendo una nube de palabras *rápido y furioso*\n",
        "\n",
        "Ahora vamos autilizar el texto tal cual lo tenemos para hacer una nube de palabras, usando solo lo que nos ofrece la biblioteca de [`wordcloud`](https://amueller.github.io/word_cloud).\n",
        "\n"
      ],
      "metadata": {
        "id": "4Bw-ohs13G6Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Primero vamos a ver la funcionalidad básica\n",
        "# de la clase WordCloud\n",
        "wordcloud.WordCloud?"
      ],
      "outputs": [],
      "metadata": {
        "id": "ekJQSgWW4gfU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "texto = '\\n'.join(df_informe.Parrafo.values)\n",
        "\n",
        "# Genera la nube de palabras\n",
        "wc = wordcloud.WordCloud().generate(texto)\n",
        "\n",
        "# Muestra la nube de palabras\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "YUtkSlWJ5Cq8",
        "outputId": "11094ab5-64bf-4dd4-ada0-6024aa2b511f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pues muy bonita pero muy inutil. El problema más grande de esta nube de palabras es que se hizo tomando en cuenta todas las palabras, y la mayoría de las que más se repiten no dan información.\n",
        "\n",
        "Vamos entonces a usar la serie de palabras de paro que nos da `spacy` con el modelo en español que bajamos."
      ],
      "metadata": {
        "id": "cFRj4XoL7Iz0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "palabras_paro = nlp.Defaults.stop_words\n",
        "\n",
        "# Genera la nube de palabras\n",
        "wc = wordcloud.WordCloud(\n",
        "    stopwords=palabras_paro\n",
        ").generate(texto)\n",
        "\n",
        "# Muestra la nube de palabras\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "PCA7RFhf7dCk",
        "outputId": "26cdbab1-ee9f-4809-c466-87813d5f6988"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y pues algo mejor, pero vemos que se siguen usando paabras que en otro contexto serían palabras significativas, pero que en un informa son muy esperables. Por ejemplo: México, o las que tengan que ver con porcentajes y con cantidades."
      ],
      "metadata": {
        "id": "CiQRQlCD9TaJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Actualizamos palabras a mano\n",
        "palabras_paro.update([\n",
        "  \"México\", \"país\", \"gobierno\",\n",
        "  \"año\", \"años\", \"mil\", \"millones\", \n",
        "  \"pesos\", \"dolares\", \"dólares\", \"ciento\"\n",
        "])\n",
        "\n",
        "# Genera la nube de palabras\n",
        "wc = wordcloud.WordCloud(\n",
        "    stopwords=palabras_paro\n",
        ").generate(texto)\n",
        "\n",
        "# Muestra la nube de palabras\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "OC4QAxcX9Sf8",
        "outputId": "4d07c264-b32d-4dee-d60f-af4a8c84aa38"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un poco mejor, pero podemos ajustar mejor los tamaños de las letras y otros detalles"
      ],
      "metadata": {
        "id": "fku611mV-bwk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Una mascara redonda de radio dado en pixeles\n",
        "radio = 200\n",
        "largo = int(1.2 * radio)\n",
        "x, y = np.ogrid[:2*largo, :2*largo]\n",
        "mascara_redonda = (x - largo) ** 2 + (y - largo) ** 2 > radio ** 2\n",
        "mascara_redonda = 255 * mascara_redonda.astype(int)\n",
        "\n",
        "\n",
        "# Genera la nube de palabras\n",
        "wc = wordcloud.WordCloud(\n",
        "    stopwords=palabras_paro,\n",
        "    max_words=100,\n",
        "    max_font_size=50,\n",
        "    background_color=\"black\",\n",
        "    #mask= mask\n",
        ").generate(texto)\n",
        "\n",
        "# Muestra la nube de palabras\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# Si te gusta lo puedes guardad\n",
        "wc.to_file(\"nube.png\")"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "LHqtxSL3-yTJ",
        "outputId": "9b6d6c14-7ac4-4c29-b644-388781087fee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usando `spacy`para extraer mejores características\n",
        "\n",
        "Vamos ahora a utilizar spacy y su capacidad para tratar tokens de forma automñatica para extraer diferentes características importantes del informe y revisar como procesar texto con spacy.\n",
        "\n",
        "Por ejemplo, vamos a ver que adjetivos utilizó el presidente en su discurso"
      ],
      "metadata": {
        "id": "eN69Zyo2Cqt3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "doc = nlp(texto)\n",
        "\n",
        "palabras = ' '.join(\n",
        "    [ \n",
        "     token.norm_ for token in doc\n",
        "     if token.is_alpha and not token.like_num and not token.is_stop and\n",
        "        not token.is_currency and token.pos_ in ['ADJ']\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Genera la nube de palabras\n",
        "wc = wordcloud.WordCloud().generate(palabras)\n",
        "\n",
        "# Muestra la nube de palabras\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "Bw7_nfjEGV6h",
        "outputId": "9ea61264-1e31-43f5-f755-10cc3a5e4ea1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Y que verbos utilizó el presidente? ¿Se puede sacar alguna conclusión? ¿Cambia si se usan los verbos en infinitivo?"
      ],
      "metadata": {
        "id": "HpvyRI8yHVoK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "verbos = ' '.join(\n",
        "  [token.norm_ for token in doc if token.pos_ in ['VERB']]\n",
        ")\n",
        "\n",
        "verbos_inf = ' '.join(\n",
        "  [token.lemma_ for token in doc if token.pos_ in ['VERB']]\n",
        ")\n",
        "\n",
        "\n",
        "# Genera la nube de palabras\n",
        "wc = wordcloud.WordCloud().generate(verbos)\n",
        "wc2 = wordcloud.WordCloud().generate(verbos_inf)\n",
        "\n",
        "# Muestra la nube de palabras\n",
        "plt.figure(figsize=(15, 15))\n",
        "plt.subplot(2,1,1)\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.subplot(2,1,2)\n",
        "plt.imshow(wc2, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 866
        },
        "id": "braENa9lHhBS",
        "outputId": "55c68f35-7cbb-4857-fa9a-0f8af2979a65"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Y su usamos puros sustantivos?"
      ],
      "metadata": {
        "id": "sBAXHVFgKC1W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "palabras = ' '.join(\n",
        "    [ \n",
        "     token.norm_ for token in doc\n",
        "     if token.is_alpha and not token.like_num and not token.is_stop and\n",
        "        not token.is_currency and token.pos_ in ['NOUN']\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Genera la nube de palabras\n",
        "wc = wordcloud.WordCloud().generate(palabras)\n",
        "\n",
        "# Muestra la nube de palabras\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "61l0TbToKXp3",
        "outputId": "6a2a1f17-874d-4937-e340-e88c849be96a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Y por nombres propios?"
      ],
      "metadata": {
        "id": "urvWSfBWKyIv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "palabras = ' '.join(\n",
        "    [ \n",
        "     token.norm_ for token in doc\n",
        "     if token.is_alpha and not token.like_num and not token.is_stop and\n",
        "        not token.is_currency and token.pos_ in ['PROPN'] and\n",
        "        token.norm_ not in ['méxico', 'nacional', 'secretaría', 'programa']\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Genera la nube de palabras\n",
        "wc = wordcloud.WordCloud().generate(palabras)\n",
        "\n",
        "# Muestra la nube de palabras\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "2YASg55uK3d-",
        "outputId": "7c2de76b-3493-49c7-9144-df9d57489633"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Y que tal si pudiermos ver los lugares que más mencionó?"
      ],
      "metadata": {
        "id": "aVihZnBNKI3r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "palabras = ' '.join(\n",
        "    [ \n",
        "     token.norm_ for token in doc\n",
        "     if token.is_alpha and not token.like_num and not token.is_stop and\n",
        "        not token.is_currency and token.ent_type_ is 'ORG' \n",
        "    ]\n",
        ")\n",
        "\n",
        "# Genera la nube de palabras\n",
        "wc = wordcloud.WordCloud().generate(palabras)\n",
        "\n",
        "# Muestra la nube de palabras\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "i8M5YvrhMpT9",
        "outputId": "0cda394f-d6c7-472f-f272-483ce667a74b"
      }
    }
  ]
}