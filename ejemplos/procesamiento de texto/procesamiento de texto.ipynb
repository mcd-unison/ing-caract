{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<p><img src=\"https://mcd.unison.mx/wp-content/themes/awaken/img/logo_mcd.png\" width=\"100\" align=\"center\"></p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Procesamiento de texto \n",
    "\n",
    "## Ingeniería de Características \n",
    "\n",
    "### Maestría en Ciencia de Datos \n",
    "### Universidad de Sonora\n",
    "\n",
    "\n",
    "\n",
    "#### Julio Waissman Vilanova (julio.waissman@unison.mx)\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "El correcto procesamiento de texto es un paso escencial para cualquier tarea de Procesamiento del Lenguaje Natural (PLN). En no pocas ocasiones, la calidad de los resultados se encuentra intimamente relacionada con ésta tarea. Sin embargo, después de la obtención de documentos (una tarea aún más ingrata) esta es una de las tareas menos glamorosa. \n",
    "\n",
    "Hay una serie de desiciones que hay que tomar desde esta etapa, y algunas veces es necesario volver a estas instancias para poder obtener un resultado satisfactorio."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Normalización\n",
    "\n",
    "### Expresiones regulares en python\n",
    "\n",
    "La limpieza de texto es un paso crucial, el cual se da a lo largo de todo el procesamiento de texto. Vamos a revisar algunos de los métodos más usuales basados en reglas, los cuales tienen que ver con el manejo de expresiones regulares. Para mayor información sobre expresiones regulares y su uso en *python* se puede consultar el material siguiente:\n",
    "\n",
    "- Un [*acordeon* del módulo `re` de *python*](https://www.dataquest.io/blog/large_files/python-regular-expressions-cheat-sheet.pdf). \n",
    "\n",
    "- Un [tutorial en español sobre el uso de `re`](https://relopezbriega.github.io/blog/2015/07/19/expresiones-regulares-con-python/)\n",
    "\n",
    "El compilador de expresiones regulares viene con varias banderas de compilación, entre las que destacan `re.I` (para ignorar el uso de mayúsculas y minúsculas); `re.S`, para que el punto signifique cualquier caracter, incluido `\\n` (muy práctico en la secuancia `.*` en multiples lineas); `re.M` que permite la búsqueda en múltiples lineas, afectando la operación de los caracteres `^` y `$`; y por último `re.X` para poder representar la expresión regular en forma *verbose* (o verborreica). \n",
    "\n",
    "Vamos a hacer algunos ejemplos de expresiones regulares para practicar. Primero, vamos a compilar y explicar algunas fórmulas que típicamente son muy útiles en PLN. \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import re\n",
    "\n",
    "email_re = re.compile(r\"\"\"\n",
    "\\b               # comienzo de delimitador de palabra\n",
    "[\\w][\\w\\.-]*     # Cualquier caracter alfanumerico seguido de uno o mas caracteres mas los signos (. -)\n",
    "@                # seguido de @\n",
    "\\w[\\w\\.-]*       # cualquier caracter alfanumerico mas los signos (.-)\n",
    "\\.               # seguido de .\n",
    "[a-zA-Z]{2,6}    # dominio de alto nivel: 2 a 6 letras en minúsculas o mayúsculas.\n",
    "\\b               # fin de delimitador de palabra\n",
    "\"\"\", re.X)\n",
    "\n",
    "url_re = re.compile(r\"\"\"\n",
    "\\b                  # delimitador de palabra\n",
    "(\\w+:\\/{2})?        # caracteres iniciales (http, https, ftp, ...) seguidos de // (uno o ninguno)\n",
    "[\\d\\w-]+            # cualquier caracter alfanumerico mas -\n",
    "(\\.[\\d\\w-]+)+       # seguido de uno o mas dominios, que empiezan con punto y siguen con caracteres\n",
    "(/\\S+)*             # cualquier serie de caracteres separados por / y sin espacios en blanco\n",
    "\\b                  # delimitador de palabra\n",
    "\"\"\", re.X) \n",
    "\n",
    "insulto_re = re.compile(r\"\"\"\n",
    "([#%&\\*\\$]{2,})     # al menos dos simbolos típicos para poner insultos\n",
    "(\\w*)               # seguidos de letras\n",
    "\"\"\", re.X)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "y a hora vamos a probar algunos de estas expresiones regulares. Esta herramienta de `nltk` es muy\n",
    "util para verificar que se armaron las expresiones regulares de forma correcta."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "texto_en_bruto =\"\"\"\n",
    "Como la mayoría sabréis, un sitio Web “normal” tiene una URL en este formato: \n",
    "http://www.ordenadores-y-portatiles.com. Habrás notado que www.ordenadores-y-portatiles.com \n",
    "sigue llegando sin ningún problema al sitio. que es diferente que\n",
    "www.ordenadores-y-portatiles.com/una%20rireccion/de%20prueba/pagina.html\n",
    "Nos podemos encontrar otros formatos como es el caso de FTP, como por ejemplo \n",
    "ftp.microsoft.com en modo comando o ftp://ftp.microsoft.com si lo ponemos en un \n",
    "navegador de Internet. Tambien se puede accesar por su IP como en  217.76.130.207, \n",
    "\n",
    "Para los correos electrónicos, si tienes uno en gmail, lo puede poner como\n",
    "juliowaissman@gmail.com, JulioWaissman@gmail.com, julio.waissman@gmail.com,\n",
    "Julio.Waissman@gmail.com, y todos te llevan al mismo lado. Igual se puede tener\n",
    "correos un poco extraños como w234QWSA.dojdnn_wsda@unison.edu.mx, y deberíamos\n",
    "poder reconocerlos (entre otros)\n",
    "\"\"\"\n",
    "\n",
    "# La utilería no funciona con expresiones regulares compiladas\n",
    "email_re_str = r\"\\b[\\w-][\\w\\.-]*@\\w[\\w\\.-]*\\.[a-zA-Z]{2,6}\\b\"\n",
    "url_re_str = r\"\\b(\\w+:\\/{2})?\\b[\\d\\w-]+(\\.[\\d\\w-]+)+(/\\S+)*\\b\"\n",
    "\n",
    "re.findall(url_re_str, texto_en_bruto)\n",
    "\n",
    "cadena, n = re.subn(email_re, \":CORREO:\", texto_en_bruto)\n",
    "cadena"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ahora, vamos a hacer algunas expresiones regulares para detectar emoticones (algo particularmente útil en tratamiento de textos en redes sociales)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "emo_gde_feliz_re = re.compile(r' [8x;:=]-?(?:\\)|\\}|\\]|>){2,}')\n",
    "emo_chi_feliz_re = re.compile(r' (?:[;:=]-?[\\)\\}\\]d>])|(?:<3)|(?:XOXO)')\n",
    "emo_gde_triste_re = re.compile(r' [x:=]-?(?:\\(|\\[|\\||\\\\|/|\\{|<){2,}')\n",
    "emo_chi_triste_re = re.compile(r' [x:=]-?[\\(\\[\\|\\\\/\\{<]')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Inventa un texto en bruto con diferentes emojis y substituye los emojis por los tokens `:BHAPPY:`, `:SHAPPY:`, `BSAD:`, `:SSAD` segun corresponda.** "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ---- Inserta tu código aqui ------"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tratamiento de texto"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ahora vamos a utilizar las expresiones regulares para tratar un conjunto de documentos, para utilizarlos posteriormente en una tarea de ciencia de datos. Vamos a usar los datos de los tweets del concurso TASS 2015 que se encuentran disponibles para el desarrollo y prueba de sistemas de análisis de sentimientos de manera libre. Para esto, vamos a procesar el corupus de entrenamiento,el cual viene en formato `xml`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import xml.etree.ElementTree as et\n",
    "import pandas as pd\n",
    "\n",
    "archivo = \"general-tweets-train-tagged.xml\"\n",
    "\n",
    "arbol = et.parse(archivo)\n",
    "raiz = arbol.getroot()\n",
    "data_dic = []\n",
    "for tweet in raiz.iter('tweet'):\n",
    "    contenido = tweet.find('content').text\n",
    "    if contenido is not None:\n",
    "        data_dic.append({\n",
    "            'texto': contenido,\n",
    "            'polaridad': tweet.find('sentiments')[0].find('value').text,\n",
    "            'id': tweet.find('tweetid').text,\n",
    "            'usuario': '@' + tweet.find('user').text,\n",
    "            'fecha': tweet.find('date').text,\n",
    "            'tópicos': '[' + ', '.join(['\"' + t.text + '\"' for t in tweet.find('topics')]) + ']'\n",
    "        })\n",
    "df_train = pd.DataFrame.from_dict(data_dic)\n",
    "display(df_train.head(10))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "y vamos a quedarnos sólo con el texto:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x_train = [documento for documento in df_train['texto'].values]\n",
    "print('Conjunto con {} entradas de tweeter para ser tratadas'.format(len(x_train)))\n",
    "\n",
    "x_train[:20]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "y el conjunto debera tener 7,218 entradas.\n",
    "\n",
    "Ahora que ya tenemos los datos de entrada es necesario tratarlos. Para esto, es muy importante que se traten de forma homogenea, y que la forma en que los tratamos sea fácil de exportar, con el fin que sea reproducible. Es por esto que el tratamiento debe de ser siempre realizado en forma de función. Vamos a tratar nuestro texto en una función `prepara_texto`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Vamos a tratar que ver que significa cada expresión regular\n",
    "\n",
    "usuarios_re = re.compile(r\"@[\\w\\d]+\")\n",
    "\n",
    "hashtags_re = re.compile(r\"#[\\w\\d]+\")\n",
    "\n",
    "remplaza_por_espacios_re = re.compile('[\\n/(){}\\[\\]\\|@,;\\.]')\n",
    "\n",
    "simbolos_a_eliminar_re = re.compile('[^\\d\\w #+_]')\n",
    "\n",
    "def prepara_texto(texto):\n",
    "    text = texto.lower()\n",
    "    \n",
    "    # Codificaciones (problemas con UTF-8, latin1, etc...)\n",
    "    text = re.sub(r'\\\\\\\\', r'\\\\', text)\n",
    "    text = re.sub(r'\\\\\\\\', r'\\\\', text)\n",
    "    text = re.sub(r'\\\\x\\w{2,2}', ' ', text)\n",
    "    text = re.sub(r'\\\\u\\w{4,4}', ' ', text)\n",
    "    text = re.sub(r'\\\\n', ' . ', text)\n",
    "\n",
    "    # Cambia e_mails, urls y usuarios por palabra clave\n",
    "    text = re.sub(email_re, '_EMAIL_', text)\n",
    "    text = re.sub(url_re, '_URL_', text)\n",
    "    text = re.sub(usuarios_re, '_USR_', text)\n",
    "    text = re.sub(hashtags_re, '_HASHTAG_', text)\n",
    "    \n",
    "    # Elimina etiquetas de marcaje tipo xml\n",
    "    # (no se requiere en este caso pero solo para dejar el tip)\n",
    "    #text = BeautifulSoup(text, \"lxml\").get_text() \n",
    "  \n",
    "    # Las palabras con letras repetidas más de 3 veces \n",
    "    # (dos veces por las personas que abusan demasiado)\n",
    "    text = re.sub(r'([a-zA-Z])\\1\\1+(\\w*)', r'\\1\\1\\2', text)\n",
    "    text = re.sub(r'([a-zA-Z])\\1\\1+(\\w*)', r'\\1\\1\\2', text)\n",
    "    \n",
    "    # Elimina simbolos\n",
    "    text = re.sub(remplaza_por_espacios_re, ' ', text)\n",
    "    text = re.sub(simbolos_a_eliminar_re, '', text)\n",
    "    \n",
    "    return text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ahora vamos a normalizar nuestros documentos"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x_train_prep = [prepara_texto(documento) for documento in x_train]\n",
    "x_train_prep[:20]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Así como estos ejemplos, existen otros casos en los cuales se pueden establecer reglas, basadas o no en \n",
    "expresiones regulares. Algunos casos son:\n",
    "\n",
    "1. Etiquetas de marcaje (Markdown, $\\LaTeX$, ...). \n",
    "\n",
    "2. Eliminación de apostrofes\n",
    "\n",
    "3. Argot y neologísmos\n",
    "\n",
    "Igualmente, la puntuación puede mantenerse en algunos casos (por ejemplo, los signos de exclamación e interrogación). Por otra parte, usuarios, url, correos electrónicos y demás, pueden ser eliminados en lugar de mantenerlos con una palabra clave (o inclusive, pueden ser mantenidos tal cual, si la base de datos es suficientemente amplia y el nombre del usuario es fundamental para inferir el contexto).\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Tokenización\n",
    "\n",
    "La definición de *token*, particularmente en su uso en PLN no es muy preciso. Sin embargo, hay un consenso establecido en entender un *token* como la ocurrencia individual de una unidad mínima de lenguaje. Esta unidad puede pertenecer a una diversidad de *tipos* de elementos gramaticales y su única restricción es que sea un bloque indivisible de texto útil para transmitir un mensaje. La pertinencia de esta división depende mucho del lenguaje y del contexto del mensaje que se está analizando.\n",
    "\n",
    "En el caso de lenguajes como el español y el inglés, la estructura del lenguaje escrito es predominantemente alfabética con el uso de logogramas para representación alternativa/compacta de números y para símbolos ([sistemas de escritura](https://en.wikipedia.org/wiki/Writing_system)). En este caso, la segmentación de un texto en *tokens* es relativamente simple, utilizando caracteres de demarcación como pueden ser los espacios en blanco y los signos de puntuación.\n",
    "\n",
    "Utilizando la biblioteca [nltk (*Natural language toolkit*)](http://www.nltk.org) de python, vamos a revisar diferentes formas de *tokenizar* a partir de este [texto](https://www.zendalibros.com/5-poemas-de-ruben-dario/) de Rubén Darío:\n",
    "\n",
    ">Junto al negro palacio del rey de la isla de Hierro (¡Oh, cruel, horrible, destierro!) ¿Cómo es que\n",
    "tú, hermana armoniosa, haces cantar al cielo gris, tu pajarera de ruiseñores, tu formidable caja musical?\n",
    "¿No te entristece recordar la primavera en que oíste a un pájaro divino y tornasol\n",
    ">\n",
    ">en el país del sol?\n",
    ">\n",
    ">En el jardín del rey de la isla de Oro (¡oh, mi ensueño que adoro!) fuera mejor que tú, armoniosa\n",
    "hermana, amaestrases tus aladas flautas, tus sonoras arpas; tú que naciste donde más lindos nacen el clavel de sangre y la rosa de arrebol,\n",
    ">\n",
    ">en el país del sol\n",
    ">\n",
    ">O en el alcázar de la reina de la isla de Plata (Schubert, solloza la Serenata…) pudieras también, hermana\n",
    "armoniosa, hacer que las místicas aves de tu alma alabasen, dulce, dulcemente, el claro de luna, los vírgenes lirios, la monja paloma y el cisne marqués. La mejor plata se funde en un ardiente crisol,\n",
    ">\n",
    ">en el país del sol\n",
    ">\n",
    ">Vuelve, pues a tu barca, que tiene lista la vela (resuena, lira, Céfiro, vuela) y parte, armoniosa\n",
    "hermana, a donde un príncipe bello, a la orilla del mar, pide liras, y versos y rosas, y acaricia sus rizos de\n",
    "oro bajo un regio y azul parasol,\n",
    ">\n",
    ">en el país del sol\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import nltk\n",
    "\n",
    "el_pais_del_sol = \"\"\"Junto al negro palacio del rey de la isla de Hierro (¡Oh, cruel, horrible, destierro!) ¿Cómo es que\n",
    "tú, hermana armoniosa, haces cantar al cielo gris, tu pajarera de ruiseñores, tu formidable caja musical?\n",
    "¿No te entristece recordar la primavera en que oíste a un pájaro divino y tornasol\n",
    "\n",
    "en el país del sol?\n",
    "\n",
    "En el jardín del rey de la isla de Oro (¡oh, mi ensueño que adoro!) fuera mejor que tú, armoniosa\n",
    "hermana, amaestrases tus aladas flautas, tus sonoras arpas; tú que naciste donde más lindos nacen el clavel de sangre y la rosa de arrebol,\n",
    "\n",
    "en el país del sol\n",
    "\n",
    "O en el alcázar de la reina de la isla de Plata (Schubert, solloza la Serenata…) pudieras también, hermana\n",
    "armoniosa, hacer que las místicas aves de tu alma alabasen, dulce, dulcemente, el claro de luna, los vírgenes lirios, la monja paloma y el cisne marqués. La mejor plata se funde en un ardiente crisol,\n",
    "\n",
    "en el país del sol\n",
    "\n",
    "Vuelve, pues a tu barca, que tiene lista la vela (resuena, lira, Céfiro, vuela) y parte, armoniosa\n",
    "hermana, a donde un príncipe bello, a la orilla del mar, pide liras, y versos y rosas, y acaricia sus rizos de\n",
    "oro bajo un regio y azul parasol,\n",
    "\n",
    "en el país del sol\"\"\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "tokens = tokenizer.tokenize(el_pais_del_sol)\n",
    "print(tokens[:60])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "tokens = tokenizer.tokenize(el_pais_del_sol)\n",
    "print(tokens[:60])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(el_pais_del_sol)\n",
    "print(tokens[:60])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(el_pais_del_sol)\n",
    "print(tokens[:60])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generalmente la *tokenización* se realiza internamente como un paso intermedio de otros métodos. Vamos a utilizar el *tokenizador* más simple para nuestro texto ya tratado"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "x_train_tokens = [tokenizer.tokenize(x) for x in x_train_prep]\n",
    "\n",
    "for i in range(5):\n",
    "    print(x_train_tokens[i])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Y ahora analicemos un poco como está funcionando nuestro tokenizador, después de normalizar los datos"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "palabras_comunes = nltk.FreqDist(t for d in x_train_tokens for t in d)\n",
    "pc_df = pd.DataFrame(palabras_comunes.most_common(50), columns=['Palabras', 'Frecuencia'])\n",
    "pc_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (20, 12)\n",
    "\n",
    "pc_df.loc[:,:].plot.bar(x='Palabras', y='Frecuencia', fontsize=20)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Como vemos, salen muchas palabras que no nos sirven para gran cosa, y eso es porque no quitamos las palabras de paro. Así que trataremos nuestros datos para quitarlas y ver como es la distribución de palabras que quedan."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "palabras_paro = nltk.corpus.stopwords.words('spanish')\n",
    "x_train_tokens_sw = [[palabra for palabra in tuit if palabra not in palabras_paro] for\n",
    "                     tuit in x_train_tokens]\n",
    "\n",
    "palabras_comunes = nltk.FreqDist(t for d in x_train_tokens_sw for t in d)\n",
    "pc_df = pd.DataFrame(palabras_comunes.most_common(50), columns=['Palabras', 'Frecuencia'])\n",
    "\n",
    "pc_df.loc[3:,:].plot.bar(x='Palabras', y='Frecuencia', fontsize=20)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "palabras_paro[-5:]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3. *Stemming*/Lematización\n",
    "\n",
    "El proceso de *Stemming*/Lematización tiene como fin reducir el tamaño del vocabulario, eliminando algunos casos relativamente simples, como puede ser plurl/singular o femenino/masculino en palabras simples. \n",
    "\n",
    "Si bien existen una cantidad importante de métodos de *stem* para inglés, existen pocos trabajos sobre esto en español. El único método de *stemming* en español bien documentado es el conocido como [*Snowball Spanish stemming algorithm](http://snowball.tartarus.org/algorithms/spanish/stemmer.html) el cual se encuentra implementado en *nltk*. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(el_pais_del_sol)\n",
    "\n",
    "stemmer = nltk.stem.snowball.SpanishStemmer()\n",
    "tokens_stem = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "print(tokens_stem[:60])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Veamos ahora que pasa si utilizamos el método de *stemming* y comparamos la dispersión de las palabras respecto a las que se generaron con el tokenizador sin más"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x_train_stem = [[stemmer.stem(token) for token in doc] for doc in x_train_tokens_sw]\n",
    "\n",
    "pc_stem = nltk.FreqDist(t for d in x_train_stem for t in d)\n",
    "pc_stem_df = pd.DataFrame(pc_stem.most_common(50), columns=['Palabras', 'Frecuencia'])\n",
    "pc_stem_df.loc[3:,:].plot.bar(x='Palabras', y='Frecuencia', fontsize=20)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}