---
title: Limpieza de datos 
subtitle: Curso Ingeniería de Características
layout: page
hero_image: https://github.com/mcd-unison/ing-caract/raw/main/docs/img/organize-banner.jpg
hero_darken: true
show_sidebar: false
---


## Imputación de datos

1. Presentación [*Dealing with Missing Data*](https://harvard-iacs.github.io/2020-CS109A/lectures/lecture19/slides/Lecture19_Missingdata.pdf) de curso *CS109A Introduction to Data Science* de Harvard.
2. Presentación [*Dealing with Missing Data*](https://cyfs.unl.edu/cyfsprojects/videoPPT/8551c12760de7027a89d14b29c26522a/151026-Enders.pdf) de *Craig Enders* de la UCLA. Muy completo.
3. Una imagen con [el resumen del proceso de imputación](https://github.com/mcd-unison/ing-caract/raw/main/docs/imagenes/missing_data.png)
4. [Missing value imputation: a review and analysis of the literature](https://github.com/mcd-unison/ing-caract/raw/main/slides/imputation-review.pdf)
5. [Manejo de valores faltantes en `pandas`](https://pandas.pydata.org/docs/user_guide/missing_data.html)
6. [Imputación con Sci-kit Learn](https://scikit-learn.org/stable/modules/impute.html)
7. [Libreta de python (Kaggle) sobre imputación](https://www.kaggle.com/parulpandey/a-guide-to-handling-missing-values-in-python)
8.  [Top Techniques to Handle Missing Values Every Data Scientist Should Know](https://www.datacamp.com/tutorial/techniques-to-handle-missing-data-values). Blog de *DataCamp*

## Detección de anomalías

1. [Una presentación inicial de detección de anomalías](http://courses.washington.edu/css581/lecture_slides/18_anomaly_detection.pdf)
2. [Otra presentación sobre técnicas específicas de detección de anomalías](https://lhchen.top/slides/AnomalyDetection/Anomaly-Detection.pdf)
3. [Recursos sobre detección de anomalías](https://github.com/yzhao062/anomaly-detection-resources)
4. [Novelty and Outlier Detection (Sci-kit Learn)](https://scikit-learn.org/stable/modules/outlier_detection.html)
5. [PyOD](https://pyod.readthedocs.io/en/latest/) y una [breve reseña](https://towardsdatascience.com/pyod-a-unified-python-library-for-anomaly-detection-3608ec1fe321) en TDS.
6. [Una libreta](https://colab.research.google.com/github/mcd-unison/ing-caract/blob/main/ejemplos/anomalias/taller_solar.ipynb) con un ejemplito sobre detección y eliminación de *outliers*.

## Análisis en componentes principales

1. [Notas sobre PCA](https://github.com/mcd-unison/ing-caract/raw/main/pdf/PCA-Standford.pdf) del curso de Andrew Ng en Stanford
2. [Principal Component Analysis](https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html). Libreta de Colab del libro [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/)
3. [Una libreta sobre PCA](https://colab.research.google.com/github/mcd-unison/ing-caract/blob/main/ejemplos/reduccion-caracteristicas/pca.ipynb) para visualización de variables.

![](https://www.explainxkcd.com/wiki/images/e/e9/data_pipeline.png)

## Métodos no lineales de reducción de características para visualización

1. [Kernel PCA](https://people.eecs.berkeley.edu/~wainwrig/stat241b/scholkopf_kernel.pdf) con su respectivo [ejemplo en `sci-kit learn`](https://scikit-learn.org/stable/auto_examples/decomposition/plot_kernel_pca.html#sphx-glr-auto-examples-decomposition-plot-kernel-pca-py)
2. [*Manifold learning*](https://scikit-learn.org/stable/modules/manifold.html) en `sci-kit learn`
3. [Libreta de colab sobre *Manifold Learning*](https://jakevdp.github.io/PythonDataScienceHandbook/05.10-manifold-learning.html) del libro [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/)
4. [Un curso de *Manifold Learning*](https://github.com/drewwilimitis/Manifold-Learning) a partir de libretas *jupyter*
5. El [repositorio/curso de GitHub de Stefan Kühn](https://github.com/cc-skuehn/Manifold_Learning) con [una presentación aceptable](https://github.com/cc-skuehn/Manifold_Learning/blob/master/Slides/Mcubed_20181016.pdf).
6. El algoritmo más conocido [*t-distributed stochastic neighbor embedding (t-SNE)*](https://cs.nyu.edu/~roweis/papers/sne_final.pdf), con una [explicación clara del algoritmo](https://www.oreilly.com/content/an-illustrated-introduction-to-the-t-sne-algorithm/) y un [muy bonito artículo interactivo para entender como hace las separaciones el método de *t-SNE*](https://distill.pub/2016/misread-tsne/)
7. El metodo de moda [*Uniform Manifold Aproximation Proyection (UMAP)*](https://arxiv.org/pdf/1802.03426.pdf) y el [enlace a la librería en python con ejemplos de aplicación](https://umap-learn.readthedocs.io/en/latest/index.html)

