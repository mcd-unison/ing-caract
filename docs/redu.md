---
title: Reducción de datos 
subtitle: Curso Ingeniería de Características
layout: page
hero_image: https://github.com/mcd-unison/ing-caract/raw/main/docs/img/eda-banner.jpg
hero_darken: true
show_sidebar: false 
---




## Selección de características

1. Coeficientes de correlación de [Pearson](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient), [Spearman](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient), [Kendall](https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient) y[$\Phi_k$](https://phik.readthedocs.io/en/latest/index.html) (con [un ejemplito](https://github.com/KaveIO/PhiK/blob/master/phik/notebooks/phik_tutorial_basic.ipynb) de como usarla).
2. [*A Literature Review of Feature Selection Techniques and Applications*](https://github.com/mcd-unison/ing-caract/raw/main/pdf/feature-selection-review.pdf)
3. [*Permutation Importance*](https://scikit-learn.org/stable/modules/permutation_importance.html) en `sci-kit learn`.
4. [Selección de características](https://topepo.github.io/caret/feature-selection-overview.html) en `caret`.

## Análisis en componentes principales

1. [Notas sobre PCA](https://github.com/mcd-unison/ing-caract/raw/main/pdf/PCA-Standford.pdf) del curso de Andrew Ng en Stanford
2. [Principal Component Analysis](https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html). Libreta de Colab del libro [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/)
3. [Una libreta sobre PCA](https://github.com/mcd-unison/ing-caract/raw/main/ejemplos/reduccion-caracteristicas/pca.ipynb) para visualización de variables.


## Métodos no lineales de reducción de características para visualización

1. [Kernel PCA](https://people.eecs.berkeley.edu/~wainwrig/stat241b/scholkopf_kernel.pdf) con su respectivo [ejemplo en `sci-kit learn`](https://scikit-learn.org/stable/auto_examples/decomposition/plot_kernel_pca.html#sphx-glr-auto-examples-decomposition-plot-kernel-pca-py)
2. [*Manifold learning*](https://scikit-learn.org/stable/modules/manifold.html) en `sci-kit learn`
3. [Libreta de colab sobre *Manifold Learning*](https://jakevdp.github.io/PythonDataScienceHandbook/05.10-manifold-learning.html) del libro [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/)
4. [Un curso de *Manifold Learning*](https://github.com/drewwilimitis/Manifold-Learning) a partir de libretas *jupyter*
5. El [repositorio/curso de GitHub de Stefan Kühn](https://github.com/cc-skuehn/Manifold_Learning) con [una presentación aceptable](https://github.com/cc-skuehn/Manifold_Learning/blob/master/Slides/Mcubed_20181016.pdf).
6. El algoritmo más conocido [*t-distributed stochastic neighbor embedding (t-SNE)*](https://cs.nyu.edu/~roweis/papers/sne_final.pdf), con una [explicación clara del algoritmo](https://www.oreilly.com/content/an-illustrated-introduction-to-the-t-sne-algorithm/) y un [muy bonito artículo interactivo para entender como hace las separaciones el método de *t-SNE*](https://distill.pub/2016/misread-tsne/)
7. El metodo de moda [*Uniform Manifold Aproximation Proyection (UMAP)*](https://arxiv.org/pdf/1802.03426.pdf) y el [enlace a la librería en python con ejemplos de aplicación](https://umap-learn.readthedocs.io/en/latest/index.html)

